{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUsBq9JyD0DV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Setting up the device for PyTorch\n",
        "# CUDA will be used if available, otherwise CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Loading the data from a .npz file\n",
        "data = np.load('5000_pos_max_norm.npz')\n",
        "\n",
        "# Extracting input and output segments from the loaded data\n",
        "input_segments = data['input_segments']\n",
        "output_segments = data['output_segments']\n",
        "\n",
        "# Reshaping the input and output segments for compatibility with the model\n",
        "# Transposing the dimensions to match expected input format for GRU\n",
        "input_segments_reshaped = np.transpose(input_segments, (2, 1, 0))\n",
        "output_segments_reshaped = np.transpose(output_segments, (2, 1, 0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset_and_get_stats(input_data, target_data, TRAIN_SPLIT=0.8, VAL_SPLIT=0.5):\n",
        "    '''\n",
        "    Split the dataset into training, validation, and testing sets.\n",
        "    The function takes in input and target data, and the split ratios.\n",
        "    It returns a dictionary containing the split datasets.\n",
        "    '''\n",
        "    # Splitting the data into training and other sets (validation + test)\n",
        "    train_data_in, other_data_in = train_test_split(input_data, train_size=TRAIN_SPLIT, shuffle=True, random_state=123)\n",
        "    train_data_out, other_data_out = train_test_split(target_data, train_size=TRAIN_SPLIT, shuffle=True, random_state=123)\n",
        "\n",
        "    # Further splitting the other_data into validation and test sets\n",
        "    val_data_in, test_data_in = train_test_split(other_data_in, train_size=VAL_SPLIT, shuffle=True, random_state=123)\n",
        "    val_data_out, test_data_out = train_test_split(other_data_out, train_size=VAL_SPLIT, shuffle=True, random_state=123)\n",
        "\n",
        "    # Compiling and returning the dataset splits\n",
        "    dataset_splits = {\n",
        "        \"train\": (train_data_in, train_data_out),\n",
        "        \"val\": (val_data_in, val_data_out),\n",
        "        \"test\": (test_data_in, test_data_out)\n",
        "    }\n",
        "    return dataset_splits\n",
        "\n",
        "# Splitting the reshaped datasets into training, validation, and testing sets\n",
        "dataset_splits = split_dataset_and_get_stats(input_segments_reshaped, output_segments_reshaped)"
      ],
      "metadata": {
        "id": "rJrNINhED3Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the split data into PyTorch DataLoader objects\n",
        "def data_to_dataloader(train_inputs, train_targets, val_inputs, val_targets, test_inputs, test_targets, batch_size=64):\n",
        "    '''\n",
        "    Converts input and target sequences into PyTorch DataLoader objects.\n",
        "    This is critical for batch processing during model training and evaluation.\n",
        "    '''\n",
        "    # Creating TensorDatasets for training, validation, and testing\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_inputs, dtype=torch.float32), torch.tensor(train_targets, dtype=torch.float32))\n",
        "    val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_inputs, dtype=torch.float32), torch.tensor(val_targets, dtype=torch.float32))\n",
        "    test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_inputs, dtype=torch.float32), torch.tensor(test_targets, dtype=torch.float32))\n",
        "\n",
        "    # Creating DataLoader objects for the datasets\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Creating DataLoader objects for the split datasets\n",
        "train_loader, val_loader, test_loader = data_to_dataloader(*dataset_splits['train'], *dataset_splits['val'], *dataset_splits['test'])"
      ],
      "metadata": {
        "id": "F21MfOrxD3M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "class TrajectoryPredictor(nn.Module):\n",
        "    '''\n",
        "    This class defines the Trajectory Predictor model using GRU.\n",
        "    The model consists of encoding and decoding GRU layers and a fully connected layer.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.5):\n",
        "        '''\n",
        "        Initialize the model with the specified parameters.\n",
        "        input_dim, hidden_dim, and output_dim define the dimensions of the network layers.\n",
        "        num_layers and dropout are used to configure the GRU layers.\n",
        "        '''\n",
        "        super(TrajectoryPredictor, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Encoding layer with dropout for regularization\n",
        "        self.gru1 = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        # Decoding layer to generate output sequence\n",
        "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for final output predictions\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward pass of the model.\n",
        "        Takes input x and produces the output through the GRU layers and fully connected layer.\n",
        "        '''\n",
        "        # Encoding\n",
        "        out, h_n = self.gru1(x)\n",
        "\n",
        "        # Decoding\n",
        "        dec_input = torch.zeros(x.size(0), 10, self.hidden_dim).to(x.device)  # Initializing decoder input\n",
        "        out, _ = self.gru2(dec_input, h_n)\n",
        "\n",
        "        # Passing through the fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Setting up model hyperparameters\n",
        "input_dim = 3   # Dimension for 3D coordinates\n",
        "hidden_dim = 64 # Number of hidden units in GRU\n",
        "output_dim = 3  # Output dimension for 3D coordinates\n",
        "num_layers = 2  # Number of GRU layers\n",
        "dropout = 0.5   # Dropout rate for regularization\n",
        "\n",
        "# Instantiating the model\n",
        "model = TrajectoryPredictor(input_dim, hidden_dim, output_dim, num_layers, dropout)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "hUsMD4MID3Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining loss function and optimizer\n",
        "criterion = nn.MSELoss() # Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer\n",
        "scheduler = StepLR(optimizer, step_size=50, gamma=0.1) # Learning rate scheduler\n",
        "\n",
        "# Defining various performance metrics\n",
        "def mse(y_true, y_pred):\n",
        "    '''Calculates Mean Squared Error between true and predicted values.'''\n",
        "    return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    '''Calculates Root Mean Squared Error between true and predicted values.'''\n",
        "    return torch.sqrt(mse(y_true, y_pred))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    '''Calculates Mean Absolute Error between true and predicted values.'''\n",
        "    return torch.abs(y_true - y_pred).mean()\n",
        "\n",
        "def r2_score(y_true, y_pred):\n",
        "    '''Calculates R-squared (coefficient of determination) score.'''\n",
        "    ss_res = mse(y_true, y_pred) * y_true.numel()\n",
        "    ss_tot = ((y_true - y_true.mean()) ** 2).sum()\n",
        "    return 1 - ss_res / ss_tot\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred, n, p):\n",
        "    '''Calculates Adjusted R-squared score.'''\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "# Training and validation loops\n",
        "n_epochs = 1000\n",
        "patience = 100\n",
        "early_stopping_counter = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        # Logging the training and validation progress\n",
        "    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.7f}, Validation Loss: {val_loss:.7f}')\n",
        "\n",
        "    # Learning Rate Scheduler Step\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        # Saving the best model state\n",
        "        torch.save(model.state_dict(), '5000_pos_max_norm_64.pth')\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= patience:\n",
        "            print('Stopping early due to lack of improvement in validation loss!')\n",
        "            break\n",
        "\n",
        "# Visualization of training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig('./5000_pos_max_norm_64_loss_plot.png', bbox_inches='tight', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "IV2tZTPuD3Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model's performance\n",
        "def evaluate_model(model, loader):\n",
        "    '''\n",
        "    Evaluate the model's performance on a given DataLoader.\n",
        "    Calculates MSE, RMSE, MAE, R2, and Adjusted R2 metrics.\n",
        "    '''\n",
        "    model.eval()\n",
        "    total_mse, total_rmse, total_mae, total_r2, total_adj_r2 = 0.0, 0.0, 0.0, 0.0, 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            total_mse += mse(targets, outputs) * inputs.size(0)\n",
        "            total_rmse += rmse(targets, outputs) * inputs.size(0)\n",
        "            total_mae += mae(targets, outputs) * inputs.size(0)\n",
        "            total_r2 += r2_score(targets, outputs) * inputs.size(0)\n",
        "            total_adj_r2 += adjusted_r2_score(targets, outputs, inputs.size(0), 3) * inputs.size(0)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    return {\n",
        "        'MSE': total_mse / total_samples,\n",
        "        'RMSE': total_rmse / total_samples,\n",
        "        'MAE': total_mae / total_samples,\n",
        "        'R2': total_r2 / total_samples,\n",
        "        'Adjusted R2': total_adj_r2 / total_samples\n",
        "    }\n",
        "\n",
        "# Loading the best model and evaluating on train, validation, and test sets\n",
        "model.load_state_dict(torch.load('5000_pos_max_norm_64.pth'))\n",
        "train_metrics = evaluate_model(model, train_loader)\n",
        "val_metrics = evaluate_model(model, val_loader)\n",
        "test_metrics = evaluate_model(model, test_loader)\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"Training Metrics:\", train_metrics)\n",
        "print(\"\\nValidation Metrics:\", val_metrics)\n",
        "print(\"\\nTest Metrics:\", test_metrics)"
      ],
      "metadata": {
        "id": "rx-3tSUAD27O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting and saving individual trajectories\n",
        "def plot_trajectories(inputs, targets, predictions, filename):\n",
        "    '''\n",
        "    Plot the input, target, and predicted trajectories in 3D.\n",
        "    Saves the plot to a specified filename.\n",
        "    '''\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plotting the trajectories\n",
        "    ax.plot(inputs[:, 0], inputs[:, 1], inputs[:, 2], label='Input Sequence', color='blue')\n",
        "    ax.plot(targets[:, 0], targets[:, 1], targets[:, 2], label='True Future Trajectory', color='green')\n",
        "    ax.plot(predictions[:, 0], predictions[:, 1], predictions[:, 2], label='Predicted Trajectory', color='red', linestyle='--')\n",
        "\n",
        "    # Setting labels and title\n",
        "    ax.set_xlabel('X Axis')\n",
        "    ax.set_ylabel('Y Axis')\n",
        "    ax.set_zlabel('Z Axis')\n",
        "    ax.set_title('3D Trajectory Prediction')\n",
        "    ax.legend()\n",
        "\n",
        "    # Saving the plot\n",
        "    plt.savefig(filename, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Generating and saving plots for a subset of test data\n",
        "for i, (inputs, targets) in enumerate(test_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    predictions = model(inputs)\n",
        "\n",
        "    # Converting tensors to numpy arrays for plotting\n",
        "    inputs_np = inputs.cpu().detach().numpy()\n",
        "    targets_np = targets.cpu().detach().numpy()\n",
        "    predictions_np = predictions.cpu().detach().numpy()\n",
        "\n",
        "    # Plotting for the first few sequences in the batch\n",
        "    for j in range(min(5, inputs_np.shape[0])):\n",
        "        plot_filename = f'trajectory_{i}_{j}.png'\n",
        "        plot_trajectories(inputs_np[j], targets_np[j], predictions_np[j], plot_filename)"
      ],
      "metadata": {
        "id": "N0niON7ED2xs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}